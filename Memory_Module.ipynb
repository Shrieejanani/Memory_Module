{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlOCIfHSKpDRvdWOP+oKiP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shrieejanani/Memory_Module/blob/main/Memory_Module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is  Prompt?\n"
      ],
      "metadata": {
        "id": "k9wp9t4UcerG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt is nothing but a combination of input,few examples and the task description which means what exactly we want the LLM to do.Eg;If we want it to act as a translator from one language to another it will do it.If we want it to act as a science tutor.it will simply act as if it is an science tutor.If we want it to act as an financial advisor,it will do that.But for it to work in that way ,you have to pass all this  details."
      ],
      "metadata": {
        "id": "D1LmR961cpP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the required packages\n",
        "\n",
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install huggingface-hub\n",
        "!pip install tiktoken\n",
        "!pip install langchain-openai"
      ],
      "metadata": {
        "id": "gSFza9DocoaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# what is Conversation Memory"
      ],
      "metadata": {
        "id": "EK78hOGzzYo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conversation Memory** is what enables chatbots to respond to our queries in a conversational manner.It allows for coherent  conversations by considering past interactions rather than treating  each query as independent.Without conversational memory,chatbots would lack  the ability  to remember and build upon previous interaction.\n",
        "\n",
        "\n",
        "By default,chatbot agents are stateles, meaning they process each incoming query as a separate input without any knowledge of past interactions.\n",
        "\n",
        "\n",
        "However in applications like chatbot,it is crucial to remember past interactions. Conversation memory facilitates this by allowing the agent to recall and utilize information from previous  conversations\n",
        "\n",
        "In summary, conversation memory is essential for chatbots to maintain context, recall  past interactions and engage in more meaningful and coherent  conversation with users."
      ],
      "metadata": {
        "id": "biUNGZcnyN3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Different Types of Memory:**\n",
        "\n",
        "Conversation Buffer Memory\n",
        "\n",
        "Conversation Buffer Window Memory\n",
        "\n",
        "Entity Memory\n",
        "\n",
        "Conversation knowledge Graph Memory\n",
        "\n",
        "Conversation summary Memory\n",
        "\n",
        "Conversation Summary Buffer Memory\n",
        "\n",
        "Conversation Token Buffer Memory\n",
        "\n",
        "Vector Store Backend-Memory/\n"
      ],
      "metadata": {
        "id": "pFfV3-B815Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai"
      ],
      "metadata": {
        "id": "jUvr2ZLf3Boj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importin required libraries\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chains.conversation.memory import(ConversationBufferMemory,\n",
        "                                                 ConversationSummaryMemory,\n",
        "                                                 ConversationBufferWindowMemory,\n",
        "                                                 )\n",
        "import tiktoken\n"
      ],
      "metadata": {
        "id": "Aefy_TzbyC4n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory"
      ],
      "metadata": {
        "id": "8S3f0jC82sjw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Tik Token?\n",
        "\n",
        "Tik Token Developed by OpenAI, is a tool used for text tokenization.\n",
        "\n",
        "\n",
        "Tokenization involves dividing a text into smaller units such as letter or words.Tik Token allows you to count tokens and estimate the cost of using the OpenAI API,which is based on token usage .It utilizes byte pair encoding(BPE),a compression algorithm that replaces frequently occuring pairs of bytes with a single byte.\n",
        "\n",
        "\n",
        "In summary, Tik TOken helps with efficient  text  processing,token counting  and cost estimation for using OPenAI's API"
      ],
      "metadata": {
        "id": "rl-q8YG94Low"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-bioK6pVMGPypYirq2PkqHm4aT3kzy\""
      ],
      "metadata": {
        "id": "MLhgy_cA4LCp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0,model_name =\"gpt-3.5-turbo-instruct\")"
      ],
      "metadata": {
        "id": "lfKm5gaO4HIb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is Memory?\n",
        "\n",
        "Chain and Agents operate as stateless, treating each query independantly,However in applications like chatbots ,its crucial to remember past interactions. The concept of memory saves that purpose."
      ],
      "metadata": {
        "id": "efg7oawd6OEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coversation Buffer Memory\n",
        "\n",
        "<font color=\"blue\">\n",
        "Imagine you're having a conversation with someone, and you want to remember what you've discussed so far.\n",
        "   <br>\n",
        "The ConversationBufferMemory does exactly that in a chatbot or similar system. It keeps a record, or \"buffer,\" of the past parts of the conversation.\n",
        "    \n",
        "<br>This buffer is an essential part of the context, which helps the chatbot generate better responses. The unique thing about this memory is that it stores the previous conversation exactly as they were, without any changes. <br><br>\n",
        "    <b>It preserves the raw form of the conversation, allowing the chatbot to refer back to specific parts accurately. In summary, the ConversationBufferMemory helps the chatbot remember the conversation history, enhancing the overall conversational experience.\n",
        "     <font>\n",
        "\n"
      ],
      "metadata": {
        "id": "aAIG1m__6yRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">\n",
        "<b>Pros of ConversationBufferMemory:</b>\n",
        "    \n",
        "    *Complete conversation history: It retains the entire conversation history, ensuring comprehensive context for the chatbot.\n",
        "    Accurate references: By storing conversation excerpts in their original form, it enables precise referencing to past interactions, enhancing accuracy.\n",
        "    *Contextual understanding: The preserved raw form of the conversation helps the chatbot maintain a deep understanding of the ongoing dialogue.\n",
        "    *Enhanced responses: With access to the complete conversation history, the chatbot can generate more relevant and coherent responses.\n",
        "     \n",
        "<font color=\"blue\">   \n",
        "<b>Cons of ConversationBufferMemory:</b>\n",
        "\n",
        "    *Increased memory usage: Storing the entire conversation history consumes memory resources, potentially leading to memory constraints.\n",
        "    *Potential performance impact: Large conversation buffers may slow down processing and response times, affecting the overall system performance.\n",
        "    *Limited scalability: As the conversation grows, the memory requirements and processing load may become impractical for extremely long conversations.\n",
        "    *Privacy concerns: Storing the entire conversation history raises privacy considerations, as sensitive or personal information may be retained in the buffer.\n",
        "<font>"
      ],
      "metadata": {
        "id": "fBxKuC0Q7FKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(llm=llm,\n",
        "                                 verbose=True,\n",
        "                                 memory=ConversationBufferMemory()\n",
        "                                 )"
      ],
      "metadata": {
        "id": "KHaS48Eu6HfI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's have a look at the prompt template that is being sent to the LLM\n",
        "\n",
        "print(conversation.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXj62ryV7spL",
        "outputId": "2a73de3c-ba4c-40d4-f1bb-a6a5b834fa70"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation(\"Good Morning AI!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4eaWVmT8DM-",
        "outputId": "8fe4096d-518c-4cb0-8d7f-b7a3eabc070b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Good Morning AI!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Good Morning AI!',\n",
              " 'history': '',\n",
              " 'response': \" Good morning, human! It's a pleasure to interact with you today. How are you feeling?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation(\"My name is Janani!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd2jUb3g8Job",
        "outputId": "eb474589-4a44-4cd0-eea5-29da8217d233"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Good Morning AI!\n",
            "AI:  Good morning, human! It's a pleasure to interact with you today. How are you feeling?\n",
            "Human: My name is Janani!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'My name is Janani!',\n",
              " 'history': \"Human: Good Morning AI!\\nAI:  Good morning, human! It's a pleasure to interact with you today. How are you feeling?\",\n",
              " 'response': \" Hello Janani! It's nice to meet you. Is there anything specific you would like to talk about or ask me? I am always happy to share my knowledge and insights with you.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I stay in chennai,India\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "C620TVYr8Rls",
        "outputId": "4d1a560c-75ad-450d-bf2c-f6d857f2c924"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Good Morning AI!\n",
            "AI:  Good morning, human! It's a pleasure to interact with you today. How are you feeling?\n",
            "Human: My name is Janani!\n",
            "AI:  Hello Janani! It's nice to meet you. Is there anything specific you would like to talk about or ask me? I am always happy to share my knowledge and insights with you.\n",
            "Human: I stay in chennai,India\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" That's interesting, Janani! Chennai is the capital city of the Indian state of Tamil Nadu. It is located on the Coromandel Coast of the Bay of Bengal and is known for its beautiful beaches, rich culture, and delicious food. Did you know that Chennai was formerly known as Madras and was established in the 17th century by the British East India Company? It is also a major hub for the automobile industry and is home to the second largest beach in the world, Marina Beach. Have you had a chance to explore your city and its many attractions?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our LLM with ConversationBufferMemory has a special ability to remember past conversation.\n",
        "\n",
        "It stores and saves the earlier interactions,allowing it to recall and use that information to make better responses.It ensures that the chatbot can keep track of the conversation history for a more coherent experience"
      ],
      "metadata": {
        "id": "xveMynPJ84td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p0YDP9K8mkb",
        "outputId": "9db2cbc1-2dc9-4a9b-eead-a340b0999990"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Good Morning AI!\n",
            "AI:  Good morning, human! It's a pleasure to interact with you today. How are you feeling?\n",
            "Human: My name is Janani!\n",
            "AI:  Hello Janani! It's nice to meet you. Is there anything specific you would like to talk about or ask me? I am always happy to share my knowledge and insights with you.\n",
            "Human: I stay in chennai,India\n",
            "AI:  That's interesting, Janani! Chennai is the capital city of the Indian state of Tamil Nadu. It is located on the Coromandel Coast of the Bay of Bengal and is known for its beautiful beaches, rich culture, and delicious food. Did you know that Chennai was formerly known as Madras and was established in the 17th century by the British East India Company? It is also a major hub for the automobile industry and is home to the second largest beach in the world, Marina Beach. Have you had a chance to explore your city and its many attractions?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "QGtYPNKx9rOU",
        "outputId": "d88e19b2-e594-4c2d-ba17-5afd4d4fd816"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Good Morning AI!\n",
            "AI:  Good morning, human! It's a pleasure to interact with you today. How are you feeling?\n",
            "Human: My name is Janani!\n",
            "AI:  Hello Janani! It's nice to meet you. Is there anything specific you would like to talk about or ask me? I am always happy to share my knowledge and insights with you.\n",
            "Human: I stay in chennai,India\n",
            "AI:  That's interesting, Janani! Chennai is the capital city of the Indian state of Tamil Nadu. It is located on the Coromandel Coast of the Bay of Bengal and is known for its beautiful beaches, rich culture, and delicious food. Did you know that Chennai was formerly known as Madras and was established in the 17th century by the British East India Company? It is also a major hub for the automobile industry and is home to the second largest beach in the world, Marina Beach. Have you had a chance to explore your city and its many attractions?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Your name is Janani, as you mentioned earlier. Is there anything else you would like to know?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVrCnZ3Q9x7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XH8b5gIf-SIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation Buffer Window Memory"
      ],
      "metadata": {
        "id": "-vMZ_hdI-VDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we have limited space in your memory to remember recent conversation we can use conversation buffer window memory.\n",
        "\n",
        "The Conversation Buffer Window Memory is like having a short-term memory that only keeps track of the most recent interactions.It intentionally drops the oldest ones to make room for the new ones.\n",
        "\n",
        "This helps manage the memory load and reduce the number of tokens used.The important thing is that it still keeps the latest parts of the conversation in their original form,without any modifications.\n",
        "\n",
        "So,it retains the most recent information for the chatbot to refer,ensuring a more efficient and up to date conversation experince.\n",
        "\n"
      ],
      "metadata": {
        "id": "COLKf86K-kXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pros of ConversationBufferWindowMemory:**\n",
        "\n",
        "    *Efficient memory utilization: It maintains a limited memory space by only retaining the most recent interactions, optimizing memory usage.\n",
        "    *Reduced token count: Dropping the oldest interactions helps to keep the token count low, preventing potential token limitations.\n",
        "    *Unmodified context retention: The latest parts of the conversation are preserved in their original form, ensuring accurate references and contextual understanding.\n",
        "    *Up-to-date conversations: By focusing on recent interactions, it allows the chatbot to stay current and provide more relevant responses.\n",
        "    \n",
        "**Cons of ConversationBufferWindowMemory:**\n",
        "\n",
        "    *Limited historical context: Since older interactions are intentionally dropped, the chatbot loses access to the complete conversation history, potentially impacting long-term context and accuracy.\n",
        "    *Loss of older information: Valuable insights or details from earlier interactions are not retained, limiting the chatbot's ability to refer back to past conversations.\n",
        "    *Reduced depth of understanding: Without the full conversation history, the chatbot may have a shallower understanding of the user's context and needs.\n",
        "    *Potential loss of context relevance: Important information or context from older interactions may be disregarded, affecting the chatbot's ability to provide comprehensive responses in certain scenarios.\n"
      ],
      "metadata": {
        "id": "EEhcR8tTBBnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation  = ConversationChain(llm=llm,\n",
        "                                  verbose = True,\n",
        "                                  memory = ConversationBufferWindowMemory(k=3))"
      ],
      "metadata": {
        "id": "ais5J75J-dLO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's have a look at the prompt template that is being sent to the LLM\n",
        "\n",
        "print(conversation.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz6ACPoTBgfX",
        "outputId": "9cbbe23b-7335-426f-b9b3-e3e2afaa4b4f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Good Morning AI!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "7uz-En_rBvzR",
        "outputId": "6038e3e2-fad4-457f-cc72-58c20d9d3ed7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Good Morning AI!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Good morning, human! It's a pleasure to interact with you today. How are you feeling?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"My name is Janani\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "iSXhiRRIB5F7",
        "outputId": "8f0bfba9-5d44-4364-949d-3023329c86a3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Good Morning AI!\n",
            "AI:  Good morning, human! It's a pleasure to interact with you today. How are you feeling?\n",
            "Human: My name is Janani\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Nice to meet you, Janani! My name is AI, which stands for Artificial Intelligence. I am a computer program designed to simulate human conversation and assist with tasks. Is there anything I can help you with today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"I stay in Chennai!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "Kl8A3_HvChlg",
        "outputId": "9ed12a8a-155e-475f-ac02-392ff0474c07"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Good Morning AI!\n",
            "AI:  Good morning, human! It's a pleasure to interact with you today. How are you feeling?\n",
            "Human: My name is Janani\n",
            "AI:  Nice to meet you, Janani! My name is AI, which stands for Artificial Intelligence. I am a computer program designed to simulate human conversation and assist with tasks. Is there anything I can help you with today?\n",
            "Human: I stay in Chennai!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" That's interesting, Janani! Chennai is the capital city of the Indian state of Tamil Nadu. It is known for its beautiful beaches, rich culture, and delicious food. Have you lived there your whole life or did you move there at some point?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM with ConversationBufferWindowMemory has a special ability to remember past conversations.\n",
        "\n",
        "It stores and saves the earlier interactions based on our choice of K which simply means the number of past conversions basically a count,allowing it to recall and use that information to make better responses . It ensures that the chatbot can keep track of the conversation history for more coherent experince."
      ],
      "metadata": {
        "id": "OcCwd2I2Dgdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJdgqj1gCqT5",
        "outputId": "d74ad480-3bd5-4930-a63c-2b3c6e547846"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Good Morning AI!\n",
            "AI:  Good morning, human! It's a pleasure to interact with you today. How are you feeling?\n",
            "Human: My name is Janani\n",
            "AI:  Nice to meet you, Janani! My name is AI, which stands for Artificial Intelligence. I am a computer program designed to simulate human conversation and assist with tasks. Is there anything I can help you with today?\n",
            "Human: I stay in Chennai!\n",
            "AI:  That's interesting, Janani! Chennai is the capital city of the Indian state of Tamil Nadu. It is known for its beautiful beaches, rich culture, and delicious food. Have you lived there your whole life or did you move there at some point?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "LdMVxYmnEjfo",
        "outputId": "398e6a78-1009-433d-95e0-056ff82d1a6b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Good Morning AI!\n",
            "AI:  Good morning, human! It's a pleasure to interact with you today. How are you feeling?\n",
            "Human: My name is Janani\n",
            "AI:  Nice to meet you, Janani! My name is AI, which stands for Artificial Intelligence. I am a computer program designed to simulate human conversation and assist with tasks. Is there anything I can help you with today?\n",
            "Human: I stay in Chennai!\n",
            "AI:  That's interesting, Janani! Chennai is the capital city of the Indian state of Tamil Nadu. It is known for its beautiful beaches, rich culture, and delicious food. Have you lived there your whole life or did you move there at some point?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Your name is Janani, as you mentioned earlier. Is there anything else you would like to know about yourself?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation Summary Memory"
      ],
      "metadata": {
        "id": "lASMQZC_FD5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the conversation BUffer Memory the length of the conversation keeps increasing which can become a problem if it becomes too large for our llm to handle.\n",
        "\n",
        "To Overcome this , we introduce Conversation Summary Memory.It keeps summary of the past conversation snippets as our history.But how does it summarize Here comes the LLM to rescue.The LLM helps in condensing or summarizing the conversation capturin the key information.\n",
        "\n",
        "So,instead of storing the entire conversation we store a summarized version.This helps manage the token count and allows the LLM to process the conversation effectively. In summary, ConversationSummary Memorykeeps a condensed of the previous conversation using the power of LLM summarization"
      ],
      "metadata": {
        "id": "MDinmW0KFL-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">\n",
        "    <b>Pros of ConversationSummaryMemory:</b>\n",
        "\n",
        "    *Efficient memory management- It keeps the conversation history in a summarized form, reducing the memory load.\n",
        "    *Improved processing- By condensing the conversation snippets, it makes it easier for the language model to process and generate responses.\n",
        "    *Avoiding maxing out limitations- It helps prevent exceeding the token count limit, ensuring the prompt remains within the processing capacity of the model.\n",
        "    *Retains important information- The summary captures the essential aspects of previous interactions, allowing for relevant context to be maintained.\n",
        "    \n",
        "\n",
        "<font>\n",
        "\n",
        "\n",
        "<font color=\"blue\">\n",
        "    <b>Cons of ConversationSummaryMemory:</b>\n",
        "\n",
        "\n",
        "    *Potential loss of detail: Since the conversation is summarized, some specific details or nuances from earlier interactions might be omitted.\n",
        "    *Reliance on summarization quality: The accuracy and effectiveness of the summarization process depend on the language model's capability, which might introduce potential errors or misinterpretations.\n",
        "    *Limited historical context: Due to summarization, the model's access to the complete conversation history may be limited, potentially impacting the depth of understanding.\n",
        "    *Reduced granularity: The summarized form may lack the fine-grained information present in the original conversation, potentially affecting the accuracy of responses in certain scenarios.\n",
        "    \n",
        "\n",
        "<font>"
      ],
      "metadata": {
        "id": "bvlZnnerG10s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation  = ConversationChain(llm  = llm,\n",
        "                                  verbose = True,\n",
        "                                  memory = ConversationSummaryMemory(llm=llm))"
      ],
      "metadata": {
        "id": "7xSyE2hYExv0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets have look at the prompt template that is being sent to the llm\n",
        "\n",
        "print(conversation.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6bzpV0nHPtS",
        "outputId": "c0815ac6-ee7c-4a06-8f33-cb784f9c4b8e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets have a look at the prompt template tht is being  sent out to LLM for our summarization task\n",
        "\n",
        "#EXAMPLE = memory = ConversationSummaryMemory(llm=llm)"
      ],
      "metadata": {
        "id": "Y9vX8Ky5HgpZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.memory.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh9gW_6wH7k1",
        "outputId": "3d650fa3-4b5d-42e5-ebc3-764fcd8f0540"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "{summary}\n",
            "\n",
            "New lines of conversation:\n",
            "{new_lines}\n",
            "\n",
            "New summary:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Good Morning AI?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "A2pJmHQtILqQ",
        "outputId": "7ad13ec3-c1da-42e5-fe5b-c3f98489becb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Good Morning AI?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Good morning! It's currently 9:00 AM here in my server room. The temperature is a comfortable 72 degrees Fahrenheit and the humidity is at 45%. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"My name is janani\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "QV0FLb43IXCf",
        "outputId": "026c4bfe-9c27-4eb3-c8be-667ea7a193d9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "The human greets the AI and the AI responds with a detailed description of its current environment. The AI then asks how it can assist the human.\n",
            "Human: My name is janani\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello janani, it's nice to meet you. I am an AI currently residing in a large data center. My main function is to process and analyze large amounts of data for various purposes. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I stay in chennai,india\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "8gjxCruOIe7a",
        "outputId": "f58360b2-0159-4c45-c4d6-57bd0721b525"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "The human greets the AI and the AI responds with a detailed description of its current environment. The AI then asks how it can assist the human, introducing itself as an AI residing in a data center with the main function of processing and analyzing data. The human, named janani, is welcomed by the AI and asked for assistance.\n",
            "Human: I stay in chennai,india\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello janani, it's nice to meet you. I am an AI residing in a data center in the United States. My main function is to process and analyze data. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN8fwO5bIqMw",
        "outputId": "ada82028-a539-412c-996f-4eec8a784cbb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The human greets the AI and the AI responds with a detailed description of its current environment. The AI then introduces itself as an AI residing in a data center with the main function of processing and analyzing data. The human, named janani, is welcomed by the AI and asked for assistance. Janani mentions that she lives in Chennai, India and the AI offers to help her with any tasks she may have.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "1EJun_lnIwt1",
        "outputId": "71fd7079-9815-498a-e126-a3c2daf2642e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "The human greets the AI and the AI responds with a detailed description of its current environment. The AI then introduces itself as an AI residing in a data center with the main function of processing and analyzing data. The human, named janani, is welcomed by the AI and asked for assistance. Janani mentions that she lives in Chennai, India and the AI offers to help her with any tasks she may have.\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Your name is janani. You mentioned that you live in Chennai, India. I am an AI residing in a data center, with my main function being to process and analyze data. How can I assist you, janani?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversation Token BUffer Memory"
      ],
      "metadata": {
        "id": "OWGBJQCoJC-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">\n",
        "\n",
        "ConversationTokenBufferMemory is a memory mechanism that stores recent interactions in a buffer within the system's memory.\n",
        "<br>\n",
        "Unlike other methods that rely on the number of interactions, this memory system determines when to clear or flush interactions based on the length of tokens used.\n",
        "<br><br>Tokens are units of text, like words or characters, and the buffer is cleared when the token count exceeds a certain threshold. By using token length as a criterion, the memory system ensures that the buffer remains manageable in terms of memory usage.\n",
        "    \n",
        "<br>This approach helps maintain efficient memory management and enables the system to handle conversations of varying lengths effectively.\n",
        "<font>"
      ],
      "metadata": {
        "id": "jca5KrUaJLHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">\n",
        "    <b>Pros of ConversationTokenBufferMemory:</b>\n",
        "\n",
        "    *Efficient memory management: By using token length instead of the number of interactions, the memory system optimizes memory usage and prevents excessive memory consumption.\n",
        "    *Flexible buffer size: The system adapts to conversations of varying lengths, ensuring that the buffer remains manageable and scalable.\n",
        "    *Accurate threshold determination: Flushing interactions based on token count provides a more precise measure of memory usage, resulting in a better balance between memory efficiency and retaining relevant context.\n",
        "    *Improved system performance: With efficient memory utilization, the overall performance of the system, including response times and processing speed, can be enhanced.\n",
        "    \n",
        "<b>Cons of ConversationTokenBufferMemory:</b>\n",
        "\n",
        "    *Potential loss of context: Flushing interactions based on token length may result in the removal of earlier interactions that could contain important context or information, potentially affecting the accuracy of responses.\n",
        "    *Complexity in threshold setting: Determining the appropriate token count threshold for flushing interactions may require careful consideration and experimentation to find the optimal balance between memory usage and context retention.\n",
        "    *Difficulty in long-term context retention: Due to the dynamic nature of token-based flushing, retaining long-term context in the conversation may pose challenges as older interactions are more likely to be removed from the buffer.\n",
        "    *Impact on response quality: In situations where high-context conversations are required, the token-based flushing approach may lead to a reduction in the depth of understanding and the quality of responses.\n",
        "<font>"
      ],
      "metadata": {
        "id": "aTHS3YqMJTQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(llm=llm,\n",
        "                                 verbose = True,\n",
        "                                 memory=ConversationTokenBufferMemory(llm=llm,max_token_limit=60))"
      ],
      "metadata": {
        "id": "_O_x-9hCI3q_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peeFAJw6JvGY",
        "outputId": "0056972e-c496-4d75-9666-43e8dcf2fe78"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"Good Morning AI!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "7y5n3_btJzpP",
        "outputId": "b5316a35-58e1-4e0a-cee1-b68520dc71e4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Good Morning AI!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Good morning, human! It's a pleasure to interact with you today. How are you feeling?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"My name is janani!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "SKYjk_raJ86z",
        "outputId": "9db703e6-b07b-43cc-ce98-b9d5d9d41514"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Good Morning AI!\n",
            "AI:  Good morning, human! It's a pleasure to interact with you today. How are you feeling?\n",
            "Human: My name is janani!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello Janani! It's nice to meet you. Is there anything specific you would like to talk about or ask me? I am always happy to share my knowledge and insights with you.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"I stay in chennai,Tamilnadu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "V8kAtVLZKLnH",
        "outputId": "9917dec4-8c4f-4810-ab16-2e88429153f3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: My name is janani!\n",
            "AI:  Hello Janani! It's nice to meet you. Is there anything specific you would like to talk about or ask me? I am always happy to share my knowledge and insights with you.\n",
            "Human: I stay in chennai,Tamilnadu\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" That's interesting! Chennai is the capital city of the Indian state of Tamil Nadu. It is located on the Coromandel Coast of the Bay of Bengal and is the fourth largest city in India. It is known for its beautiful beaches, rich cultural heritage, and delicious South Indian cuisine. Have you had a chance to explore the city?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dstzGLaBKTih",
        "outputId": "52596e64-a7b8-4644-c4da-b7257800b3d9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input =\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "vojlF9vAKYsV",
        "outputId": "51556d9a-5051-4247-d857-0ee5d4e14b1d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' I do not have access to your personal information, so I do not know your name. However, I can refer to you as \"human\" or any other name you would like me to use. Is there a specific name you would prefer?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSoXGSRWKdj_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}